{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Leo el fichero y creo 100 0particiones (NO CAMBIA LA PROGRAMACIÓN EL NUMERO DE PARTICIONES!!!!!!!!!) \n",
    "# ls = sc.textFile('data/coupon150720.csv',100)\n",
    "# si no pongo nada spark lo calcula el sólo (por cores normalmente) por ejemplo 3\n",
    "ls = sc.textFile('data/coupon150720.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'79062005698500,1,MAA,AUH,9W,9W,56.79,USD,1,H,H,0526,150904,OK,IAF0',\n",
       " u'79062005698500,2,AUH,CDG,9W,9W,84.34,USD,1,H,H,6120,150905,OK,IAF0',\n",
       " u'79062005924069,1,CJB,MAA,9W,9W,60.0,USD,1,H,H,2768,150721,OK,IAA0',\n",
       " u'79065668570385,1,DEL,DXB,9W,9W,160.63,USD,2,S,S,0546,150804,OK,INA0',\n",
       " u'79065668737021,1,AUH,IXE,9W,9W,152.46,USD,1,V,V,0501,150803,OK,INA0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no hago un collect ahora porque puede tardar. IMP no tiene por qué ser las primeras pues está en un cluster \n",
    "# ahora en mi PC solo está en mi disco duro y serán las 5 primeras\n",
    "ls.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'79062005698500,1,MAA,AUH,9W,9W,56.79,USD,1,H,H,0526,150904,OK,IAF0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# esta si te garantiza que es la primera (util para ver cabeceras)\n",
    "ls.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1232662"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.41 ms, sys: 4.13 ms, total: 12.5 ms\n",
      "Wall time: 5.43 s\n",
      "CPU times: user 3.66 ms, sys: 2.34 ms, total: 5.99 ms\n",
      "Wall time: 4.25 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1232662"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls.cache()\n",
    "%time ls.count()\n",
    "%time ls.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'IB', u'IB', u'IB', u'IB', u'IB']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numero de pasajeros de IBERIA\n",
    "a = ls.map(lambda x: x.split(\",\")[5])\n",
    "a.filter(lambda x: x=='IB').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26158"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.filter(lambda x: x=='IB').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26158"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# En programación funcional podemos anidar funciones . AHORA EN UNA LINEA\n",
    "ls.map(lambda x: x.split(\",\")[5]).filter(lambda x: x=='IB').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2828044"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# $ de IBERIA\n",
    "int(ls.map(lambda x: x.split(\",\")[5:7]).filter(lambda x: x[0]=='IB').map(lambda y: float(y[1])).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAerolineaDinero(l):\n",
    "    elems = l.split(\",\")\n",
    "    aerolinea = elems[5]\n",
    "    dinero = float(elems[6])\n",
    "    return (aerolinea, dinero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'9W', 56.79)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getAerolineaDinero(ls.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'9W', 56.79),\n",
       " (u'9W', 84.34),\n",
       " (u'9W', 60.0),\n",
       " (u'9W', 160.63),\n",
       " (u'9W', 152.46)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ya no ponemos lambda porque tenemos definida la funcion(programacion funcional)\n",
    "dineros = ls.map(getAerolineaDinero)\n",
    "dineros.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dinerosIB = dineros.filter(lambda x: x[0] == 'IB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cantidades = dinerosIB.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2828044"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(cantidades.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 26158, mean: 108.113939139, stdev: 168.011485542, max: 5239.93, min: 0.0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cantidades.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'79062005698500,1,MAA,AUH,9W,9W,56.79,USD,1,H,H,0526,150904,OK,IAF0'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcular la ruta más cara o top ten de las rutas más caras\n",
    "ls.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRutaDinero(l):            # para usar en spark sólo un parámetro\n",
    "    elems = l.split(\",\")\n",
    "    origen = elems[2]\n",
    "    destino = elems[3]\n",
    "    dinero = float(elems[6])\n",
    "    ruta = origen + \"-\" + destino # la usaré más tarde como clave\n",
    "    return (ruta, dinero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'MAA-AUH', 56.79)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getRutaDinero(ls.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'MAA-AUH', 56.79), (u'AUH-CDG', 84.34), (u'CJB-MAA', 60.0)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rutas = ls.map(getRutaDinero)\n",
    "rutas.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'MAA-AUH', 56.79),\n",
       " (u'AUH-CDG', 84.34),\n",
       " (u'CJB-MAA', 60.0),\n",
       " (u'DEL-DXB', 160.63),\n",
       " (u'AUH-IXE', 152.46)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rutas.filter(lambda x: x[1] < 7500).sortBy(lambda x: x[1], False).take(5) # primero filtro luego ordeno (más eficiente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[97] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quiero quitar repetidos (uso keys no pensar en clave unica de BBDD sino clave para agrupar)\n",
    "filtrado = rutas.filter(lambda x: x[1] < 7500)\n",
    "filtrado.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'SZF-IST', <pyspark.resultiterable.ResultIterable at 0x7f621f206c90>),\n",
       " (u'DME-RHO', <pyspark.resultiterable.ResultIterable at 0x7f621f206c50>),\n",
       " (u'KRK-OSL', <pyspark.resultiterable.ResultIterable at 0x7f621f206bd0>)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtrado.groupByKey().take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maximos = filtrado.groupByKey().mapValues(lambda vs: max(vs)) # mapvalues porque las claves no intervienen en los calculos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'BLA-MIA', 7490.33),\n",
       " (u'MIA-CCS', 7415.0),\n",
       " (u'CCS-MIA', 7415.0),\n",
       " (u'RUH-LAX', 7401.94),\n",
       " (u'DXB-MAA', 7343.27)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximos.sortBy(lambda x: x[1], False).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'SZF-IST', 77.23333333333332),\n",
       " (u'DME-RHO', 120.57857142857144),\n",
       " (u'KRK-OSL', 98.0),\n",
       " (u'OMS-AAQ', 184.14),\n",
       " (u'KBP-IST', 107.67163265306122)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La ruta con el precio medio más alto\n",
    "medias = filtrado.groupByKey().mapValues(lambda vs: sum(vs)/len(vs))\n",
    "medias.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'BLA-MEC', 5164.355),\n",
       " (u'MEC-BLA', 5164.355),\n",
       " (u'CCS-MIA', 4350.516296296297),\n",
       " (u'MIA-CCS', 4186.95),\n",
       " (u'NRT-PPT', 3751.5499999999997)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medias.sortBy(lambda x: x[1], False).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'79062005698500,1,MAA,AUH,9W,9W,56.79,USD,1,H,H,0526,150904,OK,IAF0'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Método más sofisticado de quitar outlayer (hasta ahora <7500) y ahora por categorias (ruta, aerolinea, booking class, etc...)\n",
    "ls.first() # los campos 2,3,6,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((u'MAA-AUH', u'9W', u'H'), 56.79)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getRutaALDIneroClase(l):\n",
    "    elems = l.split(\",\")\n",
    "    ruta = elems[2] + \"-\" + elems[3]\n",
    "    al = elems[4]\n",
    "    dinero = float(elems[6])\n",
    "    clase = elems[9]\n",
    "    return ((ruta,al,clase),dinero) # dinero será la clave\n",
    "getRutaALDIneroClase(ls.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.346121683250376"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defino un valor limite pro grupo\n",
    "def valorLimite(vs):\n",
    "    logs = [math.log(v+1) for v in vs]     # El más 1 es para no error log(0)(no afecta el más 1 porque no cambia la distribucuón a la hora de calcular outlayers)\n",
    "    media = sum(logs)/len(logs)\n",
    "    desv = [(x-media)**2 for x in logs]\n",
    "    var = sum(desv)/len(desv)\n",
    "    desvStd = math.sqrt(var)\n",
    "    return math.exp(media+2*desvStd) -1    # Por ejemplo además deshago el log (sólo los altos voy a eliminar)\n",
    "valorLimite([12,2,3,4,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'MAA-AUH', u'9W', u'H'), 56.79),\n",
       " ((u'AUH-CDG', u'9W', u'H'), 84.34),\n",
       " ((u'CJB-MAA', u'9W', u'H'), 60.0)]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rutas = ls.map(getRutaALDIneroClase)\n",
    "rutas.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filtraCuponesEstremos(vs):\n",
    "    vl = valorLimite(vs)\n",
    "    f = [v for v in vs if v <= vl]\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grupos = rutas.groupByKey().mapValues(filtraCuponesEstremos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Las listas vacías darían error al  alcular la media\n",
    "def mediaGrupo(vs):\n",
    "    if (len(vs) == 0):\n",
    "        return 0.0\n",
    "    else:\n",
    "        return sum(vs)/len(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'SYD-CGK', u'GA', u'G'), 6355194.0),\n",
       " ((u'COO-CDG', u'AF', u'J'), 538599.2),\n",
       " ((u'CDG-COO', u'AF', u'J'), 179628.3533333333),\n",
       " ((u'DKR-NBO', u'KQ', u'T'), 58253.593333333345),\n",
       " ((u'TSN-ICN', u'KE', u'Z'), 38655.45)]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grupos.mapValues(mediaGrupo).sortBy(lambda x: x[1], False).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'TRD-BOO', u'SK', u'V'),\n",
       "  <pyspark.resultiterable.ResultIterable at 0x7f61f7ce6110>),\n",
       " ((u'DPS-TMC', u'GA', u'N'),\n",
       "  <pyspark.resultiterable.ResultIterable at 0x7f61f7ce6290>)]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vamos a filtrar mas, por ejemplo los que tenga >10 cupones porque conjuntos de datos pequeños llevan a error\n",
    "\n",
    "grupos = rutas.groupByKey()\n",
    "grupos.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gruposfiltrados = grupos.filter(lambda x: len(x[1]) > 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = gruposfiltrados.mapValues(filtraCuponesEstremos).mapValues(mediaGrupo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'MIA-CCS', u'S3', u'S'), 7826.25),\n",
       " ((u'CCS-MIA', u'S3', u'B'), 5550.0),\n",
       " ((u'CCS-MAD', u'V0', u'Y'), 5418.098666666667),\n",
       " ((u'RUH-FRA', u'LH', u'F'), 4792.983),\n",
       " ((u'LHR-DOH', u'QR', u'F'), 4275.98923076923)]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.sortBy(lambda x: x[1], False).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'79062005698500,1,MAA,AUH,9W,9W,56.79,USD,1,H,H,0526,150904,OK,IAF0'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cuantas aeorlineas diferentes hay en el daytaset  - DISTINCT (es muy cara de recursos (incluye ordernar))\n",
    "ls.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "359"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls.map(lambda x: x.split(\",\")[4]).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# son todos los precios en dolares?\n",
    "ls.map(lambda x: x.split(\",\")[7]).distinct().count()  # en vez de count si pongo collect abuso de la memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Salvar los datos reducidos por ejemplo para tratar con R (un fichero por partición OJO!!!!!)\n",
    "ls.map(lambda x: x.split(\",\")[4]).distinct().saveAsTextFile(\"result.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diferencia entre Reduce y Groupbykey (hay que evitar el uso de este último)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'9W', 56.79)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cálculo de Ingreso total por aerolínea\n",
    "def getALIngreso(l):\n",
    "    elems = l.split(\",\")\n",
    "    al = elems[4]\n",
    "    ingreso = float(elems[6])\n",
    "    return (al,ingreso)\n",
    "getALIngreso(ls.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'9W', 56.79), (u'9W', 84.34), (u'9W', 60.0)]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als = ls.map(getALIngreso)\n",
    "als.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totales = als.groupByKey().mapValues(lambda vs: sum(vs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'GA', 20712226.56999984),\n",
       " (u'BA', 12443986.980000038),\n",
       " (u'AF', 10221507.909999428),\n",
       " (u'QF', 8358587.900000253),\n",
       " (u'LH', 7715608.339999713),\n",
       " (u'QR', 6935584.369999946),\n",
       " (u'SV', 6286299.340001837),\n",
       " (u'UA', 5151815.289999981),\n",
       " (u'JJ', 5066300.509999913),\n",
       " (u'AA', 4151996.4599999674)]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totales.sortBy(lambda x: x[1],False).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'', 478.31000000000006), (u'BE', 64073.32999999998)]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahora usaremos un ReduceByKey (Hay que incluirle una función mapreduce que aplicará a cada grupo)\n",
    "als.reduceByKey(lambda e,acum: e+acum).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'GA', 20712226.569999855),\n",
       " (u'BA', 12443986.980000002),\n",
       " (u'AF', 10221507.909999428),\n",
       " (u'QF', 8358587.9000001205),\n",
       " (u'LH', 7715608.339999718),\n",
       " (u'QR', 6935584.369999997),\n",
       " (u'SV', 6286299.340001843),\n",
       " (u'UA', 5151815.29),\n",
       " (u'JJ', 5066300.509999905),\n",
       " (u'AA', 4151996.4599999613)]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als.reduceByKey(lambda e,acum: e+acum).sortBy(lambda x: x[1],False).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.5 ms, sys: 16.3 ms, total: 30.8 ms\n",
      "Wall time: 8.82 s\n",
      "CPU times: user 9.61 ms, sys: 5.08 ms, total: 14.7 ms\n",
      "Wall time: 8.04 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'', 478.31000000000006), (u'BE', 64073.32999999998)]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparo\n",
    "%time als.groupByKey().mapValues(lambda vs: sum(vs)).take(2)\n",
    "%time als.reduceByKey(lambda e,acum: e+acum).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'S3', 5225.068852459017),\n",
       " (u'9V', 1488.673986486487),\n",
       " (u'GA', 991.2527671691715),\n",
       " (u'TN', 964.6472580645161),\n",
       " (u'7F', 668.035),\n",
       " (u'DT', 579.0006976744188),\n",
       " (u'B8', 443.295),\n",
       " (u'V0', 438.6122773044147),\n",
       " (u'NE', 430.1),\n",
       " (u'4M', 377.7557471264368)]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcular los top 10 aerolineas con mayor media de precio por empleado\n",
    "\n",
    "# con groupBY es muy sencillo\n",
    "als.groupByKey().mapValues(lambda vs: sum(vs)/len(vs)).sortBy(lambda x: x[1], False).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Con reduce hay dificultad adicional ya que la media no es acumulativa\n",
    "def calculaMediaReduce(e,acum):\n",
    "    sumaTotal = acum[0]\n",
    "    numElems = acum[1]\n",
    "    return (e[0] + sumaTotal, e[1] + numElems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'9W', (56.79, 1)), (u'9W', (84.34, 1))]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformo en tuplas para pasar luego a ReduceByKey en vez de valores \n",
    "als.map(lambda x: (x[0],(x[1],1))).take(2)  # x[0] es la clave y (x[1],1) la tupla que apsaremos a la función anterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr = als.map(lambda x: (x[0],(x[1],1))).reduceByKey(calculaMediaReduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'S3', 5225.068852459017),\n",
       " (u'9V', 1488.6739864864867),\n",
       " (u'GA', 991.2527671691723),\n",
       " (u'TN', 964.6472580645161),\n",
       " (u'7F', 668.035),\n",
       " (u'DT', 579.0006976744188),\n",
       " (u'B8', 443.295),\n",
       " (u'V0', 438.6122773044147),\n",
       " (u'NE', 430.1),\n",
       " (u'4M', 377.7557471264368)]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.mapValues(lambda x: x[0]/x[1]).sortBy(lambda x: x[1], False).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JOINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTCNIngreso(l):\n",
    "    elems = l.split(\",\")\n",
    "    tcn = elems[0]\n",
    "    ingreso = float(elems[6])\n",
    "    return (tcn, ingreso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'79062005698500', 56.79), (u'79062005698500', 84.34)]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs = sc.textFile('data/coupon150720.csv').map(getTCNIngreso)\n",
    "cs.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'79062005698500,TKTT,30,150719,FR,0.0,EUR,T,T,141025,PARA127A8,0.0,EUR,   ,EX,,150719,0.0'"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('data/transm150720.csv').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Haremos el join por el primer número\n",
    "def getTCNCanal(l):\n",
    "    elems = l.split(\",\")\n",
    "    tcn = elems[0]\n",
    "    canal = elems[8]\n",
    "    return (tcn, canal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'79062005698500', u'T'), (u'79062005924069', u'T')]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = sc.textFile('data/transm150720.csv').map(getTCNCanal)\n",
    "ts.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631405\n",
      "1232662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1232662"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cuidado un ticket (itineraio compelto) son varios cupones \n",
    "ts.cache()\n",
    "cs.cache()\n",
    "print(ts.count())\n",
    "print(cs.count())\n",
    "# ts.leftOuterJoin(cs).take(3) = cs.leftOuterJoin(ts) porque las claves estan repetidas en cs (el concepto de clave no es igual en SPARK)\n",
    "ts.leftOuterJoin(cs).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "j = ts.leftOuterJoin(cs).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'79062005994272', (u'A', 56.27)), (u'79062005994272', (u'A', 56.27))]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[581] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j.map(lambda x:x[1]) # me quedo con los valores, las claves no importan en este caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[586] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j.map(lambda x: x[1]).reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'A', 99011336.9600026),\n",
       " (u'T', 83637221.2200021),\n",
       " (u'E', 994305.880000001),\n",
       " (u'V', 843207.13),\n",
       " (u'', 345827.3100000002)]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j.map(lambda x: x[1]).reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1], False).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'79062005558463',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x7f62038f08d0>),\n",
       " (u'79065668432713',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x7f62038f0810>),\n",
       " (u'79062005501193',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x7f62038f0990>)]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejercicio= quiero claves \"reales\" sin duplicidades\n",
    "valorTS = cs.groupByKey() # puedo usar Groupbykey porque hay muchas claves\n",
    "valorTS.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'79062005558463', 0.0),\n",
       " (u'79065668432713', 286.98),\n",
       " (u'79062005501193', 272.96)]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valorTS = valorTS.mapValues(lambda vs: sum(vs))\n",
    "valorTS.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631405\n",
      "631405\n"
     ]
    }
   ],
   "source": [
    "print(ts.count())\n",
    "print(valorTS.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'79062005994272', (u'A', 112.54)),\n",
       " (u'79065668614400', (u'T', 229.51000000000002)),\n",
       " (u'79062005879562', (u'T', 99.50999999999999))]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahora tienen el mismo número\n",
    "j = ts.leftOuterJoin(valorTS)\n",
    "j.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'A', 99011336.96000284),\n",
       " (u'T', 83637221.22000207),\n",
       " (u'E', 994305.8800000008),\n",
       " (u'V', 843207.1299999999),\n",
       " (u'', 345827.31)]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ahora no groupByKey pues hay pocas claves y se cargará mucho la memoria\n",
    "j.map(lambda x: x[1]).reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1], False).take(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'79062005698500,1,MAA,AUH,9W,9W,56.79,USD,1,H,H,0526,150904,OK,IAF0',\n",
       " u'79062005698500,2,AUH,CDG,9W,9W,84.34,USD,1,H,H,6120,150905,OK,IAF0',\n",
       " u'79062005924069,1,CJB,MAA,9W,9W,60.0,USD,1,H,H,2768,150721,OK,IAA0',\n",
       " u'79065668570385,1,DEL,DXB,9W,9W,160.63,USD,2,S,S,0546,150804,OK,INA0',\n",
       " u'79065668737021,1,AUH,IXE,9W,9W,152.46,USD,1,V,V,0501,150803,OK,INA0']"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AEROLINEA que ha transportado mayor número de menores\n",
    "\n",
    "sc.textFile('data/coupon150720.csv').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTCNAero(l):\n",
    "    elems = l.split(\",\")\n",
    "    tcn = elems[0]\n",
    "    aero = elems[5]\n",
    "    return (tcn, aero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'79062005698500', u'9W'), (u'79062005698500', u'9W')]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aes = sc.textFile('data/coupon150720.csv').map(getTCNAero)\n",
    "aes.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'79062005698500,TKTT,30,150719,FR,0.0,EUR,T,T,141025,PARA127A8,0.0,EUR,   ,EX,,150719,0.0',\n",
       " u'79062005924069,TKTT,60,150719,US,0.0,USD,T,T,150719,SJC1S212D,75.38,USD,ADT,CCVI4147,,150719,0.0',\n",
       " u'79065668570385,TKTT,30,150719,AE,980.0,AED,T,T,150626,DXBFU3109,190.0,AED,CHD,EX,CA,150719,0.0',\n",
       " u'79065668737021,TKTT,0,150719,AE,560.0,AED,T,T,150719,AUHOT3128,640.0,AED,   ,CA,,150719,0.0',\n",
       " u'79062006192650,TKTT,60,150719,US,0.0,USD,T,T,150719,SEAEX3160,168.83,USD,ADT,CCVI4563,,150719,0.0']"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('data/transm150720.csv').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTCNTipo(l):\n",
    "    elems = l.split(\",\")\n",
    "    tcn = elems[0]\n",
    "    tipo = elems[13]\n",
    "    return (tcn, tipo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'79062005698500', u'   '), (u'79062005924069', u'ADT')]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tis = sc.textFile('data/transm150720.csv').map(getTCNTipo)\n",
    "tis.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Es mejor filtrar antes y evito el join con info no deseada\n",
    "fpax = tis.filter(lambda x: x[1]=='CHD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'LH', u'CHD'), (u'OS', u'CHD')]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = aes.rightOuterJoin(fpax).map(lambda x: x[1])\n",
    "j.take(2)   # la clave es la aerolinea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[750] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cambio CHD por un 1 para poder calcualr luego en el reduce\n",
    "j.mapValues(lambda x: 1).reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'JJ', 5350), (u'SV', 5073), (u'BA', 3556), (u'AF', 3378), (u'AB', 3226)]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j.mapValues(lambda x: 1).reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1], False).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Los cupones más caros que sean de niños\n",
    "def getTCNIngreso(l):\n",
    "    elems = l.split(\",\")\n",
    "    tcn = elems[0]\n",
    "    ingreso = float(elems[6])\n",
    "    return (tcn, ingreso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'79062005698500', 56.79), (u'79062005698500', 84.34)]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs = sc.textFile('data/coupon150720.csv').map(getTCNIngreso)\n",
    "cs.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'CHD', 14498.76),\n",
       " (u'CHD', 13547.31),\n",
       " (u'CHD', 9446.75),\n",
       " (u'CHD', 7709.74),\n",
       " (u'CHD', 6629.51)]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpax.join(cs).map(lambda x: x[1]).sortBy(lambda x: x[1],False).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COGROUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'79062005698500', 1), (u'79062005698500', 1), (u'79062005924069', 1)]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs = sc.textFile('data/coupon150720.csv').map(lambda x: x.split(\",\")[0]).map(lambda x: (x,1))\n",
    "cs.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'79062005698500', 1), (u'79062005924069', 1), (u'79065668570385', 1)]"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = sc.textFile('data/transm150720.csv').map(lambda x: x.split(\",\")[0]).map(lambda x: (x,1))\n",
    "ts.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'79062005994272',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7f61f7c2b850>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7f61f7c2bed0>)),\n",
       " (u'79065668614400',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7f61f7c2b110>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7f61f7c2b690>))]"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = ts.cogroup(cs).cache() # la cacheo y no la tengo que cargar de nuevo luego\n",
    "j.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'79062005994272', (1, 2)),\n",
       " (u'79065668614400', (1, 2)),\n",
       " (u'79062005879562', (1, 2))]"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j.mapValues(lambda x: (len(x[0]),len(x[1]))).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comprobar que todos son 1 en los tickets\n",
    "j.mapValues(lambda x: (len(x[0]),len(x[1]))).map(lambda x: x [1][0]).distinct().take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'79065669300201', (1, 16)),\n",
       " (u'79062005712937', (1, 15)),\n",
       " (u'79062005779716', (1, 13)),\n",
       " (u'79065668940838', (1, 12)),\n",
       " (u'79062005319376', (1, 12))]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ticket con maximo cupones\n",
    "j.mapValues(lambda x: (len(x[0]),len(x[1]))).sortBy(lambda x: x[1][1], False).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'79065669300201', 16),\n",
       " (u'79062005712937', 15),\n",
       " (u'79062005779716', 13),\n",
       " (u'79065668940838', 12),\n",
       " (u'79062005319376', 12)]"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# o mejor\n",
    "j.mapValues(lambda x: len(x[1])).sortBy(lambda x: x[1], False).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio: Histograma con fechas de antelación de compra - vuelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1441317600.0"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import strptime\n",
    "from time import mktime\n",
    "strptime('150904','%y%m%d')\n",
    "mktime(strptime('150904','%y%m%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFechaVuelo(l):\n",
    "    elems = l.split(\",\")\n",
    "    tcn = elems[0]\n",
    "    fecha = elems[12]\n",
    "    return (tcn, fecha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'79062005558463', u'150801'),\n",
       " (u'79065668432713', u'150823'),\n",
       " (u'79062005501193', u'150720')]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs = sc.textFile('data/coupon150720.csv').map(getFechaVuelo).groupByKey().mapValues(lambda x: min(x))\n",
    "cs.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFechaSalida(l):\n",
    "    elems = l.split(\",\")\n",
    "    tcn = elems[0]\n",
    "    fecha = elems[9]\n",
    "    return (tcn, fecha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'79062005698500', u'141025'),\n",
       " (u'79062005924069', u'150719'),\n",
       " (u'79065668570385', u'150626')]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = sc.textFile('data/transm150720.csv').map(getFechaSalida)\n",
    "ts.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'79062005994272', (u'150720', u'150719')),\n",
       " (u'79065668614400', (u'150724', u'150719'))]"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = cs.leftOuterJoin(ts).cache()\n",
    "j.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'79062005994272', 1.0),\n",
       " (u'79065668614400', 5.0),\n",
       " (u'79062005501193', 1.0),\n",
       " (u'79062005552377', 1.0),\n",
       " (u'79062005474691', 46.0)]"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def diferenciafechas(x):\n",
    "    return (mktime(strptime(x[0],'%y%m%d')) - mktime(strptime(x[1],'%y%m%d')))/(3600*24)\n",
    "j.mapValues(diferenciafechas).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mejor con un cogroup\n",
    "def fechamenor(ds):\n",
    "    segs=[mktime(strptime(d,'%y%m%d')) for d in ds]\n",
    "    return min(segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filtro los nulls\n",
    "fts = ts.filter(lambda x: x[1] != '')\n",
    "fcs = cs.filter(lambda x: x[1] != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'79062005994272',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7f61f7c32dd0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7f61f7c32910>)),\n",
       " (u'79065668614400',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7f61f7c328d0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7f61f7c32090>))]"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fts.cogroup(fcs).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = ts.cogroup(cs).mapValues(lambda x: (fechamenor(x[1])-fechamenor(x[0]))/(3600*24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 517.0 failed 1 times, most recent failure: Lost task 0.0 in stage 517.0 (TID 936, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2355, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2355, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1780, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 266, in mergeValues\n    for k, v in iterator:\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1907, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n  File \"<ipython-input-393-8d1f39fbb31d>\", line 1, in <lambda>\n  File \"<ipython-input-382-565f9f49dc70>\", line 3, in fechamenor\n  File \"/usr/lib64/python2.7/_strptime.py\", line 467, in _strptime_time\n    return _strptime(data_string, format)[0]\n  File \"/usr/lib64/python2.7/_strptime.py\", line 325, in _strptime\n    (data_string, format))\nValueError: time data u'' does not match format '%y%m%d'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1824)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1837)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1850)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:393)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2355, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2355, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1780, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 266, in mergeValues\n    for k, v in iterator:\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1907, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n  File \"<ipython-input-393-8d1f39fbb31d>\", line 1, in <lambda>\n  File \"<ipython-input-382-565f9f49dc70>\", line 3, in fechamenor\n  File \"/usr/lib64/python2.7/_strptime.py\", line 467, in _strptime_time\n    return _strptime(data_string, format)[0]\n  File \"/usr/lib64/python2.7/_strptime.py\", line 325, in _strptime\n    (data_string, format))\nValueError: time data u'' does not match format '%y%m%d'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-395-16b1d457345e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1299\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1301\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m    914\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 916\u001b[1;33m         \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    917\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 517.0 failed 1 times, most recent failure: Lost task 0.0 in stage 517.0 (TID 936, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2355, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2355, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1780, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 266, in mergeValues\n    for k, v in iterator:\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1907, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n  File \"<ipython-input-393-8d1f39fbb31d>\", line 1, in <lambda>\n  File \"<ipython-input-382-565f9f49dc70>\", line 3, in fechamenor\n  File \"/usr/lib64/python2.7/_strptime.py\", line 467, in _strptime_time\n    return _strptime(data_string, format)[0]\n  File \"/usr/lib64/python2.7/_strptime.py\", line 325, in _strptime\n    (data_string, format))\nValueError: time data u'' does not match format '%y%m%d'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1824)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1837)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1850)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:393)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2355, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2355, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1780, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 266, in mergeValues\n    for k, v in iterator:\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1907, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n  File \"<ipython-input-393-8d1f39fbb31d>\", line 1, in <lambda>\n  File \"<ipython-input-382-565f9f49dc70>\", line 3, in fechamenor\n  File \"/usr/lib64/python2.7/_strptime.py\", line 467, in _strptime_time\n    return _strptime(data_string, format)[0]\n  File \"/usr/lib64/python2.7/_strptime.py\", line 325, in _strptime\n    (data_string, format))\nValueError: time data u'' does not match format '%y%m%d'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "r.map(lambda x: (int(x[1]),1)).reduceByKey(lambda x,y: x+y).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
