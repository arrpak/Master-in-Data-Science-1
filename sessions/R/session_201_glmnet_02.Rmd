summary ---
title: "Regularización de la regresión con glmnet"
author: "Olivier Nuñez"
date: "1 de abril de 2016"
output: html_document
---



### Preámbulo

#### Cuando se utiliza la regularización:

* Problema de predicción con una alto número de predictores (overfitting)
* Mucho ruido en la base de datos (muchos predictores no están relacionados con la respuesta)
* Alta correlación entre los predictores (multicolinealidad)  


#### Porqué Regularizar?

* La estimación de los coeficientes puede ser muy inestable (altas varianzas)
* Permite una selección _continua_ de variables
* Subsana los problemas que conlleva la multicolinealidad



#### En qué consiste la regularización?


__Regresión lineal__:

Para $i=1,2,\dots,N$, 

$$ y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_K x_{iK} +\varepsilon_i$$

donde $\varepsilon_i \sim N(0,\sigma^2)$.

los coeficientes $\beta_k$ se estiman sin restricción.

__Regresión con regulariazión__:

los coeficientes $\beta_k$ están restringido. 

1. _Ridge regression_: $$\sum_{k=1}^K \beta_k^2 \leq t$$
2. _Lasso regression_: $$\sum_{k=1}^K \left| \beta_k\right| \leq t$$
3. _Elastic-net_: mixtura de Ridge y Lasso $$\sum_{k=1}^K \left[ \alpha\left| \beta_k\right| + (1-\alpha) \beta_k^2 \right] \leq t$$

#### Interpretación de la regularización:
* __Shrinkage__:
    + Ridge: asemeja los coeficientes de los predictores mutuamente correlacionados
    + Lasso: selecciona uno de estos predictores y descarta los demás.
* __PCA__: Penaliza las direcciones (combinaciones) de los predictores poco informativas.
* __Bayesian__: 
    + Ridge ~ los coeficientes tienen una distribución (_a priori_) gaussiana.
    + Lasso ~ los coeficientes tienen una distribución (_a priori_) de Laplace.
  
### Quick tour por glmnet

```{r,message=FALSE,warning=FALSE}
rm(list=ls()) #limpiamos la memoría
require(glmnet)

require(data.table)
require(ggplot2)
```

Un ejemplo de trenes sencillo para empezar

```{r}
######### Retrasos de trenes ###########
set.seed(4321) #fija la semilla para que el ejemplo sea reproducible.
n=1000 #tamaño de la muestra
DT=data.table(distancia=rnorm(n,500,100))
DT[,estaciones:= rpois(n,10)]
DT[,precio:=rnorm(n,50,10)]
DT=round(DT,2) 
DT
```

Ahora generamos los retrasos
```{r}
DT[,retrasos:= 1 + distancia/100 + estaciones/5 + rnorm(n,0,1)]
ggplot(DT,aes(y=retrasos,x=distancia,color=cut(estaciones,quantile(estaciones),right=TRUE))) +
  geom_smooth(se=FALSE) + scale_color_discrete("Número de estaciones")
```


Resultado de una estimación lineal
```{r}
ols <- lm(retrasos ~ . ,DT)
summary(ols)
```
La estimación es bastante buena!

Pero, en realidad el precio está muy correlacionado con los demás predictores
```{r,message=FALSE}
require(GGally)
DT[,precio:=distancia/10 - estaciones + rnorm(n,0,.1)] #precio y distancia están correlados
#ggpairs(DT)
```

Ajustamos de nuevo una regresión lineal:
```{r}
ols <- lm(retrasos ~ . ,DT)
summary(ols)
```

Para solucionar este problema utilizamos una regresión de Lasso via glmnet:
```{r}
y=DT$retrasos
x=subset(DT,select=-retrasos)
x
lasso=glmnet(as.matrix(x),y)
plot(lasso,label=TRUE)
```

Para seleccionar el coeficiente de la penalización recurrimos a cross-validation:
```{r}
cvlasso = cv.glmnet(as.matrix(x), y)
plot(cvlasso)
coef(cvlasso, s = "lambda.min")
```

Qué haría la regresión de Ridge?
```{r}
cvridge = cv.glmnet(as.matrix(x), y,alpha=0)
plot(cvridge)
coef(cvridge, s = "lambda.min")
```

Y con la elastic-net
```{r}
cvnEN = cv.glmnet(as.matrix(x), y,alpha=.5)
plot(cvnEN)
coef(cvnEN, s = "lambda.min")
```

La multicolinealidad afecta la estimación de los coeficientes de la regresión, pero no tiene poque afectar la predición.

### Overfitting

Para ilustar este problema, seguimos con el ejemplo de los trenes.
```{r}
n=100
DT=data.table(distancia=rnorm(n,0,3)) #distancias estandarizadas
DT[,media:= 2 + exp(distancia)/(1+exp(distancia))]
DT[,retrasos:= media + rnorm(n,0,.5)]
p<-ggplot(DT,aes(y=media,x=distancia)) + geom_line(size=1,color="blue4") + 
  ylim(range(DT$retraso))+xlab("distancia estandarizada") 
p
```

Para investigar la capacidad predictiva del modelo, partimos la base de datos en un conjunto de adiestrameinto y un conjunto de test:
```{r,warning=FALSE}
DT[,muestra:=sample(c("train","test"),n,prob=c(2/3,1/3),replace=TRUE)]
p<- p + geom_point(aes(y=retrasos,color=muestra),DT) 
p
```


Queremos ajustar una regresión polynomial
```{r}
poly=outer(DT$distancia,2:10,function(x,y) x^y)
DT=cbind(DT,poly=poly)
```


```{r}
ols <- lm(retrasos ~ . ,subset(DT,muestra=="train",select=-c(media,muestra)))
summary(ols)

DT[muestra=="train",fit.ols:=ols$fit] #overfitting !!
p<- p + geom_line(aes(x=distancia,y=fit.ols),data=subset(DT,muestra=="train"),col="orange3",size=1)
p
pred=predict(ols,DT[muestra=="test",])
mse=mean((pred-DT[muestra=="test",retrasos])^2)
sprintf("MSE_ols = %.3f",mse)  
```

Para regularizar esta regresión, utilizamos de nuevo glmnet 
```{r}
DT[,fit:=NULL] #se borra el fit del ols
x=subset(DT,muestra=="train",select=-c(retrasos,muestra,media,fit.ols))
y=subset(DT,muestra=="train")$retrasos
lasso = cv.glmnet(as.matrix(x), y)
plot(lasso)
coef(lasso, s = "lambda.min")
```

Y predecimos
```{r,warning=FALSE}
fit.glmnet=predict(lasso,newx=as.matrix(x),s = "lambda.min")
x.new=subset(DT,muestra=="test",select=-c(retrasos,muestra,media,fit.ols))
pred=predict(lasso,newx=as.matrix(x.new),s = "lambda.min")
mse=mean((pred-DT[muestra=="test",retrasos])^2)
sprintf("MSE_glmnet = %.3f",mse)  
DT[muestra=="train",fit.glmnet:=fit.glmnet]
p + geom_line(aes(x=distancia,y=fit.glmnet),data=subset(DT,muestra=="train"),col="green3",size=1)
```



